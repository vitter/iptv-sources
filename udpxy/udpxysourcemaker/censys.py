#!/usr/bin/env python3
"""
Censys IPä¿¡æ¯è·å–å·¥å…·
ä»censys.txtæ–‡ä»¶è¯»å–IPåœ°å€ï¼Œé€šè¿‡Censyså¹³å°è·å–UDPXYæœåŠ¡ä¿¡æ¯

é¡¹ç›®ä¸»é¡µ: https://github.com/vitter/iptv-sources
é—®é¢˜åé¦ˆ: https://github.com/vitter/iptv-sources/issues
"""

import os
import re
import csv
import json
import time
import requests
from pathlib import Path
from urllib.parse import urljoin
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_env_file(env_path=".env"):
    """åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    config = {}
    if not os.path.exists(env_path):
        return config
        
    with open(env_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ”¯æŒå¤šè¡Œå­—ç¬¦ä¸²çš„æ­£åˆ™æ¨¡å¼
    patterns = [
        # åŒå¼•å·åŒ…å›´çš„å¤šè¡Œå­—ç¬¦ä¸²
        r'([A-Z_]+)\s*=\s*"([^"]*(?:\\"[^"]*)*)"',
        # å•å¼•å·åŒ…å›´çš„å¤šè¡Œå­—ç¬¦ä¸²  
        r"([A-Z_]+)\s*=\s*'([^']*(?:\\'[^']*)*)'",
        # æ— å¼•å·çš„å•è¡Œå­—ç¬¦ä¸²
        r'([A-Z_]+)\s*=\s*([^"\'\n\r]+)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.DOTALL | re.MULTILINE)
        for key, value in matches:
            key = key.strip()
            value = value.strip()
            # å¤„ç†è½¬ä¹‰å­—ç¬¦
            value = value.replace('\\"', '"').replace("\\'", "'")
            # ç§»é™¤æœ«å°¾çš„ç©ºç™½å­—ç¬¦
            value = value.rstrip()
            config[key] = value
            # åŒæ—¶è®¾ç½®åˆ°ç¯å¢ƒå˜é‡
            os.environ[key] = value
    
    return config


def extract_json_data(html_content):
    """ä»HTMLå†…å®¹ä¸­æå–JSONæ•°æ®"""
    try:
        # é¦–å…ˆå°è¯•è§£ææ•´ä¸ªå“åº”ä¸ºJSONï¼ˆå¦‚æœæ˜¯çº¯JSONå“åº”ï¼‰
        try:
            data = json.loads(html_content)
            if isinstance(data, dict) and ('host' in data or 'services' in data):
                return data
        except json.JSONDecodeError:
            pass
        
        # æŸ¥æ‰¾åŒ…å«ç«¯å£ä¿¡æ¯çš„JSONæ•°æ®
        # é€šå¸¸åœ¨scriptæ ‡ç­¾æˆ–dataå±æ€§ä¸­
        json_patterns = [
            r'window\.__INITIAL_STATE__\s*=\s*({.*?});',
            r'window\.__NUXT__\s*=\s*({.*?});',
            r'data-props="([^"]*)"',
            r'data-page="([^"]*)"',
            r'"host":\s*({.*?"services".*?})',  # åŒ¹é…åŒ…å«hostå’Œservicesçš„å®Œæ•´å¯¹è±¡
            r'({.*?"services":\s*\[.*?\].*?})',  # åŒ¹é…åŒ…å«servicesæ•°ç»„çš„å¯¹è±¡
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, html_content, re.DOTALL)
            for match in matches:
                try:
                    # å¦‚æœæ˜¯HTMLç¼–ç çš„JSONï¼Œéœ€è¦è§£ç 
                    if match.startswith('&quot;'):
                        match = match.replace('&quot;', '"').replace('&amp;', '&')
                    
                    data = json.loads(match)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆ‘ä»¬éœ€è¦çš„æ•°æ®
                    if isinstance(data, dict) and ('services' in data or 'ports' in data or 'port' in data):
                        return data
                    elif isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and ('port' in item or 'protocol' in item):
                                return {'services': data}
                except json.JSONDecodeError:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–JSONï¼Œå°è¯•ç›´æ¥æœç´¢ç«¯å£ä¿¡æ¯
        # æ”¹è¿›çš„ç«¯å£æœç´¢æ¨¡å¼
        port_patterns = [
            # åœ¨JSONç»“æ„ä¸­æœç´¢udpxyç›¸å…³çš„ç«¯å£
            r'"port":\s*(\d+)[^}]*"vendor":\s*"udpxy"',
            r'"vendor":\s*"udpxy"[^}]*"port":\s*(\d+)',
            r'"port":\s*(\d+)[^}]*"product":\s*"udpxy"',
            r'"product":\s*"udpxy"[^}]*"port":\s*(\d+)',
            # æŸ¥æ‰¾Serverå¤´ä¸­çš„udpxyä¿¡æ¯å¯¹åº”çš„ç«¯å£
            r'"port":\s*(\d+)[^}]*"Server"[^}]*"udpxy',
            r'"Server"[^}]*"udpxy[^}]*"port":\s*(\d+)',
            # é€šç”¨æ¨¡å¼
            r'udpxy.*?port["\s:]*(\d+)',
            r'(\d+).*?udpxy',
        ]
        
        ports = []
        for pattern in port_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                try:
                    port = int(match)
                    if 1000 <= port <= 65535:  # æœ‰æ•ˆç«¯å£èŒƒå›´
                        ports.append(port)
                except ValueError:
                    continue
        
        if ports:
            return {'udpxy_ports': list(set(ports))}
            
    except Exception as e:
        logger.error(f"æå–JSONæ•°æ®æ—¶å‡ºé”™: {e}")
    
    return None


def extract_host_info(html_content):
    """ä»HTMLå†…å®¹ä¸­æå–ä¸»æœºçš„è¯¦ç»†ä¿¡æ¯"""
    info = {
        'dns': '',
        'country': '',
        'city': '',
        'province': '',
        'isp': ''
    }
    
    try:
        # æ–¹æ³•1: ä»JSONæ•°æ®ä¸­æå–ä¿¡æ¯
        json_patterns = [
            r'window\.__INITIAL_STATE__\s*=\s*({.*?});',
            r'window\.__NUXT__\s*=\s*({.*?});',
            r'data-props="([^"]*)"',
            r'({.*?"ip".*?"location".*?})',
            r'({.*?"dns".*?"location".*?})',
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, html_content, re.DOTALL)
            for match in matches:
                try:
                    # å¦‚æœæ˜¯HTMLç¼–ç çš„JSONï¼Œéœ€è¦è§£ç 
                    if match.startswith('&quot;'):
                        match = match.replace('&quot;', '"').replace('&amp;', '&')
                    
                    # å°è¯•è§£æJSON
                    data = json.loads(match)
                    
                    # æå–DNSä¿¡æ¯
                    if not info['dns']:
                        if 'dns' in data and 'names' in data['dns']:
                            names = data['dns']['names']
                            if names and len(names) > 0:
                                info['dns'] = names[0]
                    
                    # æå–åœ°ç†ä½ç½®ä¿¡æ¯
                    if 'location' in data:
                        location = data['location']
                        if 'country' in location:
                            info['country'] = location['country']
                        if 'city' in location:
                            info['city'] = location['city']
                        if 'province' in location:
                            info['province'] = location['province']
                    
                    # æå–è¿è¥å•†ä¿¡æ¯
                    if 'whois' in data and 'network' in data['whois']:
                        network = data['whois']['network']
                        if 'name' in network:
                            info['isp'] = network['name']
                    
                except (json.JSONDecodeError, KeyError):
                    continue
        
        # æ–¹æ³•2: å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        if not info['dns']:
            dns_patterns = [
                r'"dns":\s*\{[^}]*"names":\s*\[\s*"([^"]+)"',
                r'"forward_dns":[^}]*"([^"]+\.[a-zA-Z]{2,})"',
                r'"hostname":\s*"([^"]+)"'
            ]
            for pattern in dns_patterns:
                match = re.search(pattern, html_content)
                if match:
                    info['dns'] = match.group(1)
                    break
        
        if not info['country']:
            country_pattern = r'"country":\s*"([^"]+)"'
            match = re.search(country_pattern, html_content)
            if match:
                info['country'] = match.group(1)
        
        if not info['city']:
            city_pattern = r'"city":\s*"([^"]+)"'
            match = re.search(city_pattern, html_content)
            if match:
                info['city'] = match.group(1)
        
        if not info['province']:
            province_pattern = r'"province":\s*"([^"]+)"'
            match = re.search(province_pattern, html_content)
            if match:
                info['province'] = match.group(1)
        
        if not info['isp']:
            isp_patterns = [
                r'"whois":[^}]*"network":[^}]*"name":\s*"([^"]+)"',
                r'"network":[^}]*"name":\s*"([^"]+)"'
            ]
            for pattern in isp_patterns:
                match = re.search(pattern, html_content)
                if match:
                    info['isp'] = match.group(1)
                    break
        
        return info
        
    except Exception as e:
        logger.debug(f"æå–ä¸»æœºä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return info


def extract_forward_dns(html_content):
    """ä»HTMLå†…å®¹ä¸­æå–Forward DNSä¿¡æ¯ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰"""
    host_info = extract_host_info(html_content)
    return host_info['dns']


def extract_udpxy_info(html_content, ip):
    """ä»HTMLå†…å®¹ä¸­æå–UDPXYç›¸å…³ä¿¡æ¯"""
    udpxy_info = {
        'ports': [],
        'urls': [],
        'dns': '',
        'country': '',
        'city': '',
        'province': '',
        'isp': ''
    }
    
    try:
        # æå–ä¸»æœºè¯¦ç»†ä¿¡æ¯
        host_info = extract_host_info(html_content)
        udpxy_info.update(host_info)
        
        # æå–JSONæ•°æ®
        json_data = extract_json_data(html_content)
        
        if json_data:
            # ä»JSONæ•°æ®ä¸­æå–ç«¯å£ä¿¡æ¯
            services = []
            
            # å¤„ç†ä¸åŒçš„JSONç»“æ„
            if 'host' in json_data and 'services' in json_data['host']:
                # Censys APIçš„æ ‡å‡†å“åº”æ ¼å¼
                services = json_data['host']['services']
            elif 'services' in json_data:
                # ç›´æ¥åŒ…å«servicesçš„æ ¼å¼
                services = json_data['services']
            
            # éå†æ‰€æœ‰æœåŠ¡
            for service in services:
                if isinstance(service, dict):
                    port = service.get('port')
                    software = service.get('software', [])
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯udpxyæœåŠ¡
                    for sw in software:
                        if isinstance(sw, dict) and (
                            sw.get('vendor', '').lower() == 'udpxy' or 
                            sw.get('product', '').lower() == 'udpxy'
                        ):
                            if port and port not in udpxy_info['ports']:
                                udpxy_info['ports'].append(port)
                                udpxy_info['urls'].append(f"http://{ip}:{port}")
                                logger.info(f"  ä»JSONä¸­å‘ç°UDPXYæœåŠ¡: ç«¯å£ {port}")
                            break
            
            # å¦‚æœæœ‰udpxy_portså­—æ®µï¼ˆä»æ­£åˆ™æå–çš„ï¼‰
            if 'udpxy_ports' in json_data:
                for port in json_data['udpxy_ports']:
                    if port not in udpxy_info['ports']:
                        udpxy_info['ports'].append(port)
                        udpxy_info['urls'].append(f"http://{ip}:{port}")
                        logger.info(f"  ä»æ­£åˆ™è¡¨è¾¾å¼å‘ç°UDPXYæœåŠ¡: ç«¯å£ {port}")
        
        # å¦‚æœæ²¡æœ‰ä»JSONä¸­æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥ä»HTMLä¸­æœç´¢
        if not udpxy_info['ports']:
            # æœç´¢udpxyç›¸å…³çš„ç«¯å£ä¿¡æ¯
            port_patterns = [
                r'udpxy.*?(\d{4,5})',
                r'(\d{4,5}).*?udpxy',
                r'"port":\s*(\d+).*?udpxy',
                r'udpxy.*?"port":\s*(\d+)',
            ]
            
            for pattern in port_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    port = int(match)
                    if 1000 <= port <= 65535 and port not in udpxy_info['ports']:
                        udpxy_info['ports'].append(port)
                        udpxy_info['urls'].append(f"http://{ip}:{port}")
        
        # æå–Forward DNS
        udpxy_info['dns'] = extract_forward_dns(html_content)
        
    except Exception as e:
        logger.error(f"æå–UDPXYä¿¡æ¯æ—¶å‡ºé”™: {e}")
    
    return udpxy_info


def fetch_censys_data(ip, session):
    """è·å–æŒ‡å®šIPçš„Censysæ•°æ®"""
    try:
        # ä½¿ç”¨Censyså¹³å°çš„APIç«¯ç‚¹ï¼ˆä¸æµè§ˆå™¨è®¿é—®ç›¸åŒçš„ç«¯ç‚¹ï¼‰
        url = f"https://platform.censys.io/hosts/{ip}?_data=routes/hosts.$id"
        
        # æ·»åŠ æ›´å¤šéšæœºåŒ–çš„å¤´ä¿¡æ¯æ¥ç»•è¿‡æ£€æµ‹
        session.headers.update({
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Pragma': 'no-cache',
        })
        
        response = session.get(url, timeout=30)
        
        logger.info(f"è¯·æ±‚çŠ¶æ€ç : {response.status_code} for IP {ip}")
        logger.info(f"å“åº”å¤´: {dict(response.headers)}")
        logger.info(f"å“åº”å‰200å­—ç¬¦: {response.text[:200]}")
        
        if response.status_code == 200:
            # ç®€å•ç²—æš´åœ°åªä¿ç•™ASCIIå­—ç¬¦ï¼Œå¿½ç•¥æ‰€æœ‰Unicodeå­—ç¬¦
            content = response.text
            # åªä¿ç•™ASCIIå­—ç¬¦ï¼Œå…¶ä»–å­—ç¬¦ç”¨ç©ºæ ¼æ›¿æ¢
            content = ''.join(char if ord(char) < 128 else ' ' for char in content)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°CloudflareæŒ‘æˆ˜
            if "Just a moment" in content or "cf-mitigated" in str(response.headers):
                logger.warning(f"é‡åˆ°Cloudflareé˜²æŠ¤ for IP {ip} - éœ€è¦æ›´æ–°cookieæˆ–å¢åŠ å»¶è¿Ÿ")
                return None
            
            logger.info(f"æˆåŠŸè·å– {ip} çš„æ•°æ®ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return content
            
        elif response.status_code == 403:
            logger.warning(f"è®¿é—®è¢«æ‹’ç» (403) for IP {ip} - cookieå¯èƒ½è¿‡æœŸ")
            return None
        elif response.status_code == 429:
            logger.warning(f"è¯·æ±‚é™åˆ¶ (429) for IP {ip} - éœ€è¦å¢åŠ å»¶è¿Ÿ")
            # å¯¹äº429é”™è¯¯ï¼Œé¢å¤–ç­‰å¾…æ›´é•¿æ—¶é—´
            retry_after = int(response.headers.get('retry-after', '30'))
            extra_delay = min(60, max(15, retry_after))
            logger.info(f"â° é‡åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {extra_delay} ç§’åç»§ç»­...")
            try:
                time.sleep(extra_delay)
            except KeyboardInterrupt:
                logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­å»¶è¿Ÿç­‰å¾…")
                raise
            return None
        else:
            logger.warning(f"æœªçŸ¥çŠ¶æ€ç  {response.status_code} for IP {ip}")
            return None
        
    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚ {ip} å¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.error(f"å¤„ç† {ip} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None


def write_to_csv(csv_path, ip, port, url, dns, country='', city='', province='', isp=''):
    """å°†æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    file_exists = csv_path.exists()
    
    # ç®€å•ç²—æš´åœ°è¿‡æ»¤æ‰€æœ‰éASCIIå­—ç¬¦
    def clean_text(text):
        if isinstance(text, str):
            return ''.join(char if ord(char) < 128 else ' ' for char in text)
        return str(text)
    
    # æ¸…ç†æ‰€æœ‰å­—æ®µ
    ip = clean_text(ip)
    port = clean_text(str(port))
    url = clean_text(url)
    dns = clean_text(dns)
    country = clean_text(country)
    city = clean_text(city)
    province = clean_text(province)
    isp = clean_text(isp)
    
    with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥æ ‡é¢˜è¡Œ
        if not file_exists:
            writer.writerow(['ip', 'port', 'url', 'dns', 'country', 'city', 'province', 'isp'])
        
        writer.writerow([ip, port, url, dns, country, city, province, isp])


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Censys IPä¿¡æ¯è·å–å·¥å…·')
    parser.add_argument('--input', '-i', default='censys.txt', 
                       help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: censys.txt)')
    parser.add_argument('--output', '-o', default='censys.csv',
                       help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ (é»˜è®¤: censys.csv)')
    parser.add_argument('--delay', '-d', type=float, default=2.0,
                       help='è¯·æ±‚é—´éš”æ—¶é—´(ç§’) (é»˜è®¤: 2.0)')
    parser.add_argument('--proxy', '-p', type=str, default=None,
                       help='ä»£ç†æœåŠ¡å™¨ (æ ¼å¼: http://ip:port æˆ– socks5://ip:port)')
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env_file()
    
    # è·å–å¿…è¦çš„é…ç½®
    cookie = os.getenv('CENSYS_COOKIE', '')
    user_agent = os.getenv('CENSYS_USER_AGENT', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0')
    
    if not cookie:
        logger.error("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®CENSYS_COOKIE")
        return
    
    # æ¸…ç†cookieä¸­çš„éASCIIå­—ç¬¦ï¼Œé˜²æ­¢ç¼–ç é—®é¢˜
    cookie = ''.join(char if ord(char) < 128 else '' for char in cookie)
    user_agent = ''.join(char if ord(char) < 128 else '' for char in user_agent)
    
    # è®¾ç½®æ–‡ä»¶è·¯å¾„
    input_file = Path(args.input)
    output_file = Path(args.output)
    
    if not input_file.exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        return
    
    # è¯»å–IPåˆ—è¡¨
    with open(input_file, 'r', encoding='utf-8') as f:
        ips = [line.strip() for line in f if line.strip()]
    
    logger.info(f"å…±æ‰¾åˆ° {len(ips)} ä¸ªIPåœ°å€")
    
    # åˆ›å»ºä¼šè¯ï¼Œä½¿ç”¨Firefoxæµè§ˆå™¨ç‰¹å¾ï¼ˆåŒ¹é…ç”¨æˆ·çš„æµè§ˆå™¨ï¼‰
    session = requests.Session()
    session.headers.update({
        'User-Agent': user_agent,
        'Accept': '*/*',
        'Accept-Encoding': 'gzip, deflate, br, zstd',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Cookie': cookie,
        'Connection': 'keep-alive',
        'Host': 'platform.censys.io',
        'Priority': 'u=4',
        'Referer': 'https://platform.censys.io/home',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'TE': 'trailers',
        # æ·»åŠ Cloudflareéœ€è¦çš„Client Hintså¤´
        'Sec-CH-UA': '"Firefox";v="141", " Not A;Brand";v="99", "Mozilla";v="141"',
        'Sec-CH-UA-Mobile': '?0',
        'Sec-CH-UA-Platform': '"Linux"',
        'Sec-CH-UA-Arch': '"x86"',
        'Sec-CH-UA-Bitness': '"64"',
        'Sec-CH-UA-Full-Version': '"141.0"',
        'Sec-CH-UA-Platform-Version': '"6.5.0"',
        'Sec-CH-UA-Full-Version-List': '"Firefox";v="141.0", " Not A;Brand";v="99.0.0.0", "Mozilla";v="141.0"',
    })
    
    # é…ç½®ä»£ç†ï¼ˆå¦‚æœæä¾›äº†ä»£ç†å‚æ•°ï¼‰
    if args.proxy:
        proxy_dict = {
            'http': args.proxy,
            'https': args.proxy
        }
        session.proxies.update(proxy_dict)
        logger.info(f"ğŸŒ ä½¿ç”¨ä»£ç†: {args.proxy}")
        
        # ç®€åŒ–çš„ä»£ç†æµ‹è¯•ï¼ˆé¿å…æš´éœ²ä»£ç†ä½¿ç”¨ç—•è¿¹ï¼‰
        try:
            # åªåšä¸€ä¸ªç®€å•çš„è¿æ¥æµ‹è¯•ï¼Œä¸è·å–IPä¿¡æ¯
            test_response = session.head('https://www.google.com', timeout=5)
            if test_response.status_code in [200, 301, 302]:
                logger.info(f"âœ… ä»£ç†è¿æ¥æ­£å¸¸")
            else:
                logger.warning("âš ï¸ ä»£ç†æµ‹è¯•å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•")
        except Exception as e:
            logger.warning(f"âš ï¸ ä»£ç†è¿æ¥å¼‚å¸¸: {e}ï¼Œä½†å°†ç»§ç»­å°è¯•")
    else:
        logger.info("ğŸ”— ä½¿ç”¨ç›´æ¥è¿æ¥ï¼ˆæ— ä»£ç†ï¼‰")
    
    # å¤„ç†æ¯ä¸ªIP
    processed_count = 0
    success_count = 0
    
    for i, ip in enumerate(ips, 1):
        logger.info(f"å¤„ç† IP {i}/{len(ips)}: {ip}")
        
        try:
            # è·å–æ•°æ®
            html_content = fetch_censys_data(ip, session)
            
            if html_content:
                # æå–UDPXYä¿¡æ¯
                udpxy_info = extract_udpxy_info(html_content, ip)
                
                if udpxy_info['ports']:
                    # ä¸ºæ¯ä¸ªç«¯å£å†™å…¥ä¸€è¡Œæ•°æ®
                    for j, (port, url) in enumerate(zip(udpxy_info['ports'], udpxy_info['urls'])):
                        write_to_csv(output_file, ip, port, url, udpxy_info['dns'], 
                                   udpxy_info['country'], udpxy_info['city'], 
                                   udpxy_info['province'], udpxy_info['isp'])
                        logger.info(f"  å‘ç°UDPXYæœåŠ¡: {url} (DNS: {udpxy_info['dns']}, {udpxy_info['city']}, {udpxy_info['country']})")
                        success_count += 1
                else:
                    # å³ä½¿æ²¡æœ‰æ‰¾åˆ°UDPXYæœåŠ¡ï¼Œä¹Ÿè®°å½•IPå’Œå…¶ä»–ä¿¡æ¯
                    write_to_csv(output_file, ip, '', '', udpxy_info['dns'],
                               udpxy_info['country'], udpxy_info['city'], 
                               udpxy_info['province'], udpxy_info['isp'])
                    logger.info(f"  æœªå‘ç°UDPXYæœåŠ¡ (DNS: {udpxy_info['dns']}, {udpxy_info['city']}, {udpxy_info['country']})")
            else:
                # è¯·æ±‚å¤±è´¥ï¼Œè®°å½•IP
                write_to_csv(output_file, ip, '', '', '', '', '', '', '')
                logger.warning(f"  è¯·æ±‚å¤±è´¥")
            
            processed_count += 1
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(ips):
                logger.info(f"â° ç­‰å¾… {args.delay} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªIP...")
                try:
                    time.sleep(args.delay)
                except KeyboardInterrupt:
                    logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
                    break
                
        except Exception as e:
            logger.error(f"å¤„ç† {ip} æ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•IP
            write_to_csv(output_file, ip, '', '', '', '', '', '', '')
            continue
    
    logger.info(f"å¤„ç†å®Œæˆï¼å…±å¤„ç† {processed_count}/{len(ips)} ä¸ªIPï¼Œå‘ç° {success_count} ä¸ªUDPXYæœåŠ¡")
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
